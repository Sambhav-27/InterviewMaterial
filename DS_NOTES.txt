- quick sort worst case T(n)= T(n-1) + O(n)
- if we use median elemnet as pivot in quick sort which takes time O(n) the complexity would still be nlogn. T(n)= 2T(n/2) + O(n)
- Given an unsorted array. The array has this property that every element in array is at most k distance from its position in sorted array where k is a positive integer smaller than size of array.
The heaps sort with O(nlogk)
- counting sort is not comparison based.
- Any comparison based sorting algorithm can be made stable by using position as a criteria when two elements are compared.

-	for (int i = n; i > 0; i /= 2)
     	for (int j = 0; j < i; j++)
	count++;

	T(n) = O(n + n/2 + n/4 + ..+1) = O(n)

- for (int i = 0; i < n; i++)
     for (int j = i; j > 0; j--)
        count = count + 1;

	Theta(n) = 1+2+3+4+...n-1 = n(n-1)/2= theta(n*n)

- Tower of Hanoi: T(n) = 2T(n – 1) + 1
- Let w(n) and A(n) denote respectively, the worst case and average case running time  then: A(n) = O(W(n)) 

- nlogn < n^3/2 < n^logn < 2^n : check by taking a bigger example.
- Best time of bubble sort is O(N): when input is sorted by taking a bool flag.
- The tightest lower bound on the number of comparisons, in the worst case, for comparison-based sorting is of the order of NlogN

- In a modified merge sort, the input array is splitted at a position one-third of the length(N) of the array. 
then : T(N) = T(N/3) + T(2N/3) + N == N(logN base 3/2)

- int i = 0, j = 0;
    for(; i < n; ++i)
        while(j < n && arr[i] < arr[j])
            j++;
Time: O(n) . j is not initialized for every value of i.

-	 for(i = 1; i < n; i *= 2) : logn

 	for(i = n; i > -1; i /= 2) :  never terminates

- log(n!) = theta(n log n). Reason is Stirling's approximation log n! = nlogn - (loge)n + O(logn)

-when we say that an algorithm X is asymptotically more efficient than Y : X will be a better choice for all inputs except small inputs

- In quick sort, for sorting n elements, the (n/4)th smallest element is selected as pivot using an O(n) time algorithm. 
 then: T(n) = T(n/4) + T(3n/4) + cn which is nlogn

- int fun2(int n)
{
    if (n <= 1) return n;
    return fun2(n-1) + fun2(n-1);
}
time: O(2^n)

-To find minimum and maximum element out of n numbers, we need to have at least (3n/2-2) comparisons. T(n) = 2T(n/2) + 2 where 2 is for comparing the minimums as well the maximums of the left and right subarrays 
On solving, T(n) = 1.5n - 2. 

- You have an array of n elements. Suppose you implement quicksort by always choosing the central element of the array as the pivot. 
then still worst case: n*n

-double foo (int n)
{
    int i;
    double sum;
    if (n = = 0) return 1.0;
    else
    {
        sum = 0.0;
        for (i = 0; i < n; i++)
            sum += foo (i);
        return sum;
    }
}
space: O(n) as there can be atmost O(n) active funs at a time.

- Insertion sort runs in Theta(n + f(n)) time, where f(n) denotes the number of inversion initially present in the array being sorted. 
n inversion is a pair (ai, aj) such that i < j and ai > aj

- randomized quicksort can still be O(n*n)

- Kruskal: O(ElogE) flloyd warshall O(n^3) bellman ford O(VE)
- Master's theorem

- if input is already sorted: quicksort- O(n*n) insertion sort O(n)
- Radix sort


- to find missing number in a list of n distinct integers
	missing_number = a[i] ^ i for all i
- Selection sort makes O(n) swaps which is minimum among all sorting algorithms mentioned above.

- You have to sort 1 GB of data with only 100 MB of available main memory. 
The data can be sorted using external sorting which uses merging technique. This can be done as follows: 1. Divide the data into 10 groups each of size 100. 2. Sort each group and write them to disk. 3. Load 10 items from each group into main memory. 4. Output the smallest item from the main memory to disk. Load the next item from the group whose item was chosen. 5. Loop step #4 until all items are not outputted. The step 3-5 is called as merging technique.

- when all array elements are same(thus sorted) the insertion sort is linear.
-Merge Sort works better than quick sort if data is accessed from slow sequential memory.
Merge sort outperforms heap sort in most of the practical situations.

- steps of insertion and selection sort
- insertion sort is stable and Online - can sort a list at runtime; is good for small numbers.

-Quick Sort is also a cache friendly sorting algorithm as it has good locality of reference when used for arrays. Quick Sort is also tail recursive, therefore tail call optimizations is done.

- best case: insertion, bubble N; selection n*n
- heap sort is not divide and conquer. Huffman time: nlogn
- majority element in array can be found in logn


-- A full binary tree (sometimes proper binary tree or 2-tree or strictly binary tree) is a tree in which every node other than the leaves has two children. i.e. no node has 1 child.
complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible.(in a heap)
Every full tree is not complete and every complete tree is not full.

- in heap sort like storing of tree in array. if tree is skewed then array size would be 2^n -1

-no of leaf nodes= 1+ no of internal nodes with 2 children.
Don't forget in postorder root occurs at last.

-- bst worst case time for search,delete,insert= O(n)
We are given a set of n distinct elements and an unlabeled binary tree with n nodes. In how many ways can we populate the tree with the given set so that it becomes a binary search tree? Ans=1
if n element are inserted in bst in different orders than inorder traversal will always be same. It's the height that changes.
height of root=0; level of root=1;

--------------------------------------------------------
- 
